Restart of English language-learning. July 2021
===============================================
* run-2 -- work with a flawed copy of tranche-1 only

This flawed copy contains:
* 3026 articles
* 426941 sentences
* 8133834 words


Databases in the `~/data` directory
===================================

* r2-en_pairs.rdb -- just pair counts for guten-tranche-1 only.
     Above is missing 400+ files. There were some crashes.
     This DB includes MI for pairs.

* r2-mpg_parse.rdb -- MPG disjunct counts.
     This includes the MM^T marginals. w/o the shapes, though.

* r2-mpg-trim-10-2-1.rdb
* r2-mpg-trim-20-4-3.rdb
* r2-mpg-trim-40-8-5.rdb
     Trimmed version of above, with Shape MMT marginals.



Notes
=====

Compute MI stats for  r2-en_pairs.rdb --

```
Elapsed time to load ANY-link pairs: 696 secs
Support: found num left= 113809 num right= 113865 in 344 secs
Finished left norm marginals in 426 secs
Finished right norm marginals in 382 secs
Done with wild-card count N(x,*) and N(*,y) in 817 secs
Done 110000 of 113809 outer loops in 144 secs, pairs=9351757 (5338.4 pairs/sec)
Done computing 9684723 pair MI's in 1846 secs
Finished with MI computations; this took 1.421 hours.
Summary Report for Correlation Matrix Link Grammar ANY link Word Pairs
Left type: WordNode    Right Type: WordNode    Pair Type: EvaluationLink
Rows: 113809 Columns: 113865
Size: 9684723 non-zero entries of 12958861785 possible
Fraction non-zero: 7.4734E-4 Sparsity (-log_2): 10.386
Total observations: 230004595.  Avg obs per pair: 23.749
Entropy Total: 18.029   Left: 9.8981   Right: 9.7612
Total MI: 1.6307

                 Left         Right     Avg-left     Avg-right
                 ----         -----     --------     ---------
Support (l_0)  2.1263E+4    2.4050E+4
Count   (l_1)  2.8642E+6    3.2863E+6     134.7        136.6
Length  (l_2)  3.1925E+5    3.5502E+5     15.01        14.76
RMS Count      3.1896E+5    3.5472E+5     15.00        14.75
```

----------------------------------------------------

MPG parsing ...
wall-clock time 370 minutes for 3028 files so 7 seconds/file
total of 6 hours wall-clock time.
   .. seems to be faster than pair-counting.
2304:10 CPU time.  So 47 CPU-secs/file -- total of 38 hours CPU

RAM usage: 32.3g  29.5g so this is huge.

(ListLink . 9912398)
(Connector . 214265)
(ConnectorSeq . 7350903)
(Section .  8145420)
(EvaluationLink . 9912398)
(WordNode . 130905)

(display (monitor-storage storage-node))
Connected to `rocks:///home/ubuntu/data//r2-mpg_parse.rdb`
Database contents:
  Next aid: 35666394
  Atoms/Links/Nodes a@: 35666393 l@: 35535384 n@: 130924
  Keys/Incoming/Hash k@: 38224220 i@: 85829605 h@: 0

So 35666393 Atoms = 35M Atoms, or 830 Bytes/Atom resident RAM usage.

Disk usage:
$ du -s r2-mpg_parse.rdb
4304176 r2-mpg_parse.rdb

so 124 Bytes/Atom when on disk. Wooot! (without the disjunct marginals)

-------------------------
(gc-stats)
$2 = ((gc-time-taken . 7243681057538) (heap-size . 92037120)
(heap-free-size . 75968512) (heap-total-allocated . 5581286791152)
(heap-allocated-since-gc . 2248480) (protected-objects . 46) (gc-times .
120588))

So: 60 millisec/gc  and total 7243 seconds in GC, so 2 hours flat.
out of 38 hours CPU time, so about 5% of total CPU time is in GC.

Churned through 5581 terabytes RAM in the process. Yikes! That's
1.8 terbytes per file!  Holy cow! That seems completely excessive!
WTF!  ... each file has hundreds of sentences, so that's a dozen GB
per sentence! Sure, planar parsing is a complicated algo, but really???

-------------------------
Compute disjunct marginals.
run `3-mst-parsing/compute-mst-marginals.sh.`

Elapsed time to load csets: 1008 secs
Finished left norm marginals in 3969 secs
Finished left totals in 503 secs
Finished column (left) norm averages in 2714 secs
Done storing 7350903 left-wilds in 2190 secs
Finished right norm marginals in 424 secs
Finished right totals in 10 secs
Finished row (right) norm averages in 40 secs
Done storing 110388 right-wilds in 20 secs
Finished mmt norm marginals in 1330 secs
Finished mmt totals in 7 secs
Done storing 7350903 left-wilds in 1032 secs

Summary Report for Correlation Matrix Word-Disjunct Pairs (Connector
Sets)
Left type: WordNode    Right Type: ConnectorSeq    Pair Type: Section
Wildcard: (ListLink (ctv 0 0 1.00324e+07)
  (AnyNode "cset-word")
  (AnyNode "cset-disjunct"))
Rows: 110388 Columns: 7350903
Size: 8145420 non-zero entries of 811451480364 possible
Fraction non-zero: 1.0038E-5 Sparsity (-log_2): 16.604
Total observations: 10032415.0  Avg obs per pair: 1.2317
No MI statistics are present; run compute-mi to get them.

                 Left         Right     Avg-left     Avg-right
                 ----         -----     --------     ---------
Support (l_0)  33.62        8.9559E+4
Count   (l_1)  114.7        1.2295E+5     3.413        1.373
Length  (l_2)  41.54        1140.         1.236        1.2724E-2
RMS Count      33.31        1089.         .9909        1.2156E-2

MM^T support=8145420.0     count=1151160207.0     entropy=15.815
No M^TM data present

real    221m6.395s
user    304m6.116s
sys     83m52.293s

$ du -s r2-mpg_parse.rdb
5251780 r2-mpg_parse.rdb

Rocks: initial aid=43127689 (is num atoms plus one, so...)
Average of 125 Bytes/atom

------------------------------------------------------
Above seems huge so trim, according to the master README

guile -l ${COMMON_DIR}/cogserver.scm
   ; (cog-rocks-open "rocks:///home/ubuntu/data/r2-mpg_parse.rdb")
   (define pca (make-pseudo-cset-api))
   (define psa (add-pair-stars pca))
   (psa 'fetch-pairs)        ;; Load the dataset
   ; Elapsed time to load csets: 1745 secs
   (cog-close storage-node)  ;; Avoid accidental corruption

   (cog-open storage-node)
   (define psc (add-support-compute psa))
   (psc 'cache-all)          ;; compute subtotals

Finished left norm marginals in 30757 secs   ; 8.5 hours
Finished left totals in 12103 secs           ; 3.4 hours
Finished right norm marginals in 418 secs
again:
Finished left norm marginals in 27678 secs
Finished left totals in 10465 secs
Finished right norm marginals in 579 secs

   (define storage-node
      (RocksStorageNode "rocks:///home/ubuntu/data/r2-mpg-supp.rdb"))
	(define stp (make-store psc))
   (stp 'store-all)
Done storing 7350903 left-wilds in 3535 secs
Done storing 110388 right-wilds in 25 secs
Done storing 8145420 pairs in 2254 secs

   (cog-close storage-node)

   (define fsa (add-subtotal-filter psa 40 8 5 #f)) ;; The filter itself
   (define lfa (add-linkage-filter fsa))
   (define storage-node
      (RocksStorageNode "rocks:///home/ubuntu/data/r2-mpg-trim-40-8-5.rdb"))
   (cog-open storage-node)
   (define fso (make-store lfa))
   (fso 'store-all-elts)     ;; Do NOT store the marginals!
Too many heap sections: Increase MAXHINCR or MAX_HEAP_SECTS
again:
Done storing 52791 pairs in 11 secs
   (cog-close storage-node)

   (define fsa (add-subtotal-filter psa 20 4 3 #f)) ;; The filter itself
   (define lfa (add-linkage-filter fsa))
   (define storage-node
      (RocksStorageNode "rocks:///home/ubuntu/data/r2-mpg-trim-20-4-3.rdb"))
   (cog-open storage-node)
   (define fso (make-store lfa))
   (fso 'store-all-elts)     ;; Do NOT store the marginals!
Done storing 105393 pairs in 24 secs
   (cog-close storage-node)

   (define fsa (add-subtotal-filter psa 10 2 1 #f)) ;; The filter itself
   (define lfa (add-linkage-filter fsa))
   (define storage-node
      (RocksStorageNode "rocks:///home/ubuntu/data/r2-mpg-trim-10-2-1.rdb"))
   (cog-open storage-node)
   (define fso (make-store lfa))
   (fso 'store-all-elts)     ;; Do NOT store the marginals!
Done storing 309654 pairs in 68 secs
   (cog-close storage-node)

---------
run compute-mst-marginals.sh for each of the three files above.

* r2-mpg-trim-40-8-5.rdb

Rows: 6029 Columns: 98102
Size: 171922 non-zero entries of 591456958 possible
Fraction non-zero: 2.9068E-4 Sparsity (-log_2): 11.748
Total observations:  4006152.0  Avg obs per pair: 23.302
No MI statistics are present; run compute-mi to get them.

                 Left         Right     Avg-left     Avg-right
                 ----         -----     --------     ---------
Support (l_0)  45.55        2573.
Count   (l_1)  1708.        6.6260E+4     37.49        25.75
Length  (l_2)  380.5        2834.         8.352        1.101
RMS Count      290.3        2620.         6.373        1.018

MM^T support=171922.0     count=6842054908.0     entropy=2.1110

* r2-mpg-trim-20-4-3.rdb

Rows: 10216 Columns: 191449
Size: 343153 non-zero entries of 1955842984 possible
Fraction non-zero: 1.7545E-4 Sparsity (-log_2): 12.477
Total observations:  4892090.0  Avg obs per pair: 14.256
No MI statistics are present; run compute-mi to get them.

                 Left         Right     Avg-left     Avg-right
                 ----         -----     --------     ---------
Support (l_0)  74.81        5014.
Count   (l_1)  1721.        7.8050E+4     23.00        15.57
Length  (l_2)  332.1        2763.         4.439        .5511
RMS Count      267.4        2608.         3.574        .5202

MM^T support=343153.0     count=8418062550.0     entropy=3.8061

* r2-mpg-trim-10-2-1.rdb

Rows: 18997 Columns: 542181
Size: 1012198 non-zero entries of 10299812457 possible
Fraction non-zero: 9.8273E-5 Sparsity (-log_2): 13.313
Total observations:  6607826.0  Avg obs per pair: 6.5282
No MI statistics are present; run compute-mi to get them.

                 Left         Right     Avg-left     Avg-right
                 ----         -----     --------     ---------
Support (l_0)  123.4        1.3267E+4
Count   (l_1)  1578.        9.6517E+4     12.78        7.275
Length  (l_2)  264.9        2609.         2.146        .1967
RMS Count      224.4        2517.         1.818        .1898

MM^T support=1012198.0     count=10427173042.0     entropy=6.6185

---------

Gram classification

Going into
* r2-gram-shape-40-disinfo-3.0-4.rdb

scheme@(gram-class)> (define psa star-obj)
scheme@(gram-class)> (gram-classify-greedy-disinfo psa 3.0 4)
After trimming, 6029 words left, out of 6029
Sorting in 2.010 seconds
Start greedy-agglomeration of 6029 words

Right off the bat -- lots of crappy merges. Ouch.
Dist of 5 might be better.

In fact, above is so bad, its halted.
---------Bingo! Dist=3.7201 for word "of" -- "to"
---------Bingo! Dist=6.0441 for class "of to" -- "in"
---------Bingo! Dist=3.2455 for class "of to" -- "he"
---------Bingo! Dist=4.0717 for class "of to" -- "it"
---------Bingo! Dist=4.8130 for class "of to" -- "that"
---------Bingo! Dist=4.0295 for class "of to" -- "said"


* r2-gram-shape-40-disinfo-5.0-4.rdb

 (gram-classify-greedy-disinfo star-obj 5.0 4)

Yikes, no better -- this fails to merge a lot of plausible words.
But then it finds two good things, OK ... andthen after that,
it will merge any old crap.   What's going on? Why is it doing that?

---------Bingo! Dist=5.5520 for word "be" -- "have"
---------Bingo! Dist=5.0712 for class "be have" -- "him"
---------Bingo! Dist=7.3089 for class "be have" -- "do"
---------Bingo! Dist=5.1829 for class "be have" -- "all"
---------Bingo! Dist=5.4233 for class "be have" -- "her"

So.. what's going wrong? obviously, this is a "small dataset" still,
surprising.  So explore by hand.

./run-gram-cogserver.sh
(define pmi (add-symmetric-mi-compute star-obj))
(define (get-mi wa wb) (pmi 'mmt-fmi wa wb))
(define (ent swa swb)
	(format #t "~5f\n" (get-mi (Word swa) (Word swb))) *unspecified*)

(ent "of" "to")  3.720 -- looks reasonable.
(ent "of" "in")  4.443
(ent "to" "in")  3.019

(ent "of" "of") 7.460  -- a baseline, of sorts.
(ent "to" "to") 6.848
(ent "in" "in") 5.699

(ent "of" "he") -1.74  -- as expected.
(ent "of" "it") -1.44
(ent "of" "that") 0.619
(ent "of" "said") -1.43

(ent "to" "he") 0.449  -- higher than expected but still low
(ent "to" "it") -0.18
(ent "to" "that") 0.935
(ent "to" "said") 0.070

(ent "in" "he") 1.348 -- higher than expected but still low.
(ent "in" "it") 0.255
(ent "in" "that") 1.517
(ent "in" "said") -0.53

(ent "he" "he") 3.968  -- pathetically low .. small dataset, so OK.
(ent "it" "it") 2.405
(ent "that" "that") 3.664
(ent "said" "said") 3.223

So what the heck is the merger doing???

make-merger

FRAC-FUN is the fraction to merge
   ; The fraction to merge is a linear ramp, starting at zero
   ; at the cutoff, and ramping up to one when these are very
   ; similar.
   (define (mi-fraction WA WB)
      (define milo (min (get-self-mi WA) (get-self-mi WB)))
      (define fmi (get-mi WA WB))
      (/ (- fmi CUTOFF) (- milo CUTOFF)))

Rewrite above for convenience:
   (define (frac swa swb)
      (define WA (Word swa))
      (define WB (Word swb))
      (define CUTOFF 3.0)
      (define milo (min (get-mi WA WA) (get-mi WB WB)))
      (define fmi (get-mi WA WB))
      (/ (- fmi CUTOFF) (- milo CUTOFF)))
   (define (pfrac swa swb)
      (format #t "~5f\n" (frac swa swb)) *unspecified*)

For cutoff of 3.0, we get
(pfrac "of" "to") 0.187   -- yuck too low!
(pfrac "of" "in") 0.535   -- higher than expected...
(pfrac "to" "in") 0.007   -- well, barely above cutoff so not surprising!?

These lead to the above:
(ent "of" "to")  3.720
(ent "of" "in")  4.443
(ent "to" "in")  3.019

(ent "of" "of") 7.460
(ent "to" "to") 6.848
(ent "in" "in") 5.699

Conclude: maybe the frac fun is pessimisitic, or gives poor weighting.
... what should it be? What's the theory?

anyway, what happens next...? From make-merger

; (define STARS star-obj)
; (define psu (add-support-compute STARS))
; (define cls (STARS 'make-cluster WA WB))
; (start-cluster psu cls WA WB FRAC-FN NOISE MRG-CON)

So MRG-CON is ... #t and this is hard-coded in make-disinfo.
NOISE is ... 4  aka ZIPF in gram-classify-greedy-disinfo
MIN-OBS is 4, given in argument

greedy-grow throws away a word if discard? is true
trim-and-rank ignores word if discard-margin? is true

(define pmi (add-symmetric-mi-compute star-obj))
(define (get-mi wa wb) (pmi 'mmt-fmi wa wb))

   (define (mi-fraction WA WB)
      (define CUTOFF 3.0)
      (define milo (min (get-mi WA WA) (get-mi WB WB)))
      (define fmi (get-mi WA WB))
      (/ (- fmi CUTOFF) (- milo CUTOFF)))

(define (start-clu swa swb)
   (define STARS star-obj)
   (define WA (Word swa))
   (define WB (Word swb))
   (define psu (add-support-compute STARS))
   (define cls (STARS 'make-cluster WA WB))
   (start-cluster psu cls WA WB mi-fraction 4.0 #t))

Likewise ... merge-into-cluster ...

So let's see what happens.
(start-clu "of" "to")
------ Create: Merged 7620 sections in 4.000 secs; 1905.0 scts/sec
------ Create: Revised 7620 shapes in 8.000 secs; 952.50 scts/sec
------ Create: cleanup 1 in 3.000 secs; 0.3333 ops/sec

(star-obj 'make-cluster (Word "of") (Word "to"))
(define of-to (WordClassNode "of to"))

(get-mi of-to of-to)
"Numerical overflow"

(define (prj swa)
	(format #t "~5f\n" (get-mi of-to (Word swa))) *unspecified*)

(define psu (add-support-compute star-obj))
(psu 'clobber)
(psu 'set-right-marginals of-to)

(EvaluationLink
  (PredicateNode "*-Direct Sum Wild (csetâŠ•cross-section)")
  (WordClassNode "of to")
  (AnyNode "right-wild-direct-sum"))

(define ptc (add-transpose-compute star-obj))
(ptc 'set-mmt-marginals of-to)

(get-mi of-to of-to) 10.009

(prj "of") 5.637  -- oh no -- why so high??
(prj "to") 5.261  -- oh no -- why so high?
(prj "in") 6.044
(prj "he") 1.937
(prj "it") 1.629
(prj "that") 2.970
(prj "said") 1.900

OK, so clearly, the projection failed. One would expect that after
merger, what is left over in the unmerged connectors would have a
low MI. Is this a bug in the code, or is this a theoretical failure?
Given that the merge fraction was so low...

Oh, hang on, lets double-check:

(ent "of" "of") 6.148 -- so that's less than before ....
(ent "to" "to") 6.013

Crap I forgot to do this; however `make-merger` does do this correctly.

(psu 'clobber)
(ptc 'clobber)
(psu 'set-right-marginals of-to)
(psu 'set-right-marginals (Word "of"))
(psu 'set-right-marginals (Word "to"))

(ptc 'set-mmt-marginals of-to)
(ptc 'set-mmt-marginals (Word "of"))
(ptc 'set-mmt-marginals (Word "to"))

(get-mi of-to of-to) 10.009 - same as before
(ent "of" "of") 11.70
(ent "to" "to") 11.92


(prj "of") 8.411 -- yikes -- even higher!
(prj "to") 8.216
(prj "in") 6.044 -- no change from before
(prj "he") 1.937 --  "
(prj "it") 1.629 --  "
(prj "that") 2.970
(prj "said") 1.900

(ent "of" "to") -inf.0
(ent "of" "in") 5.294
(ent "of" "he") -4.44
(ent "of" "it") -3.34
(ent "of" "that") 1.012
(ent "of" "said") -3.25
(ent "to" "in") 3.558
(ent "to" "he") 0.874
(ent "to" "it") -0.25
(ent "to" "that") 1.432
(ent "to" "said") -1.97
(ent "in" "in") 5.694


See diary for extended discussion.

(define pmi (add-symmetric-mi-compute star-obj))
(define (get-mi wa wb) (pmi 'mmt-fmi wa wb))
(define (ent swa swb)
	(format #t "~5f\n" (get-mi (Word swa) (Word swb))) *unspecified*)

(define (jp swa swb)
	(format #t "~8g\n" (pmi 'mmt-joint-prob (Word swa) (Word swb))) *unspecified*)

; Compute the total entropy for two words. (pre-merger)
(define (rmi swa swb)
	(define wa (Word swa))
	(define wb (Word swb))
	(define tot-prob (+
		(pmi 'mmt-joint-prob wa wa)
		(* 2 (pmi 'mmt-joint-prob wa wb))
		(pmi 'mmt-joint-prob wb wb)))

	(define tot-mi (+
		(* (pmi 'mmt-joint-prob wa wa) (pmi 'mmt-fmi wa wa))
		(* 2 (pmi 'mmt-joint-prob wa wb) (pmi 'mmt-fmi wa wb))
		(* (pmi 'mmt-joint-prob wb wb) (pmi 'mmt-fmi wb wb))))

	(format #t "Tot prob=~8g tot mi=~8g  fmi=~6f\n"
		tot-prob tot-mi (/ tot-mi tot-prob))
	*unspecified*)

========================================================================
OK, restart.

* r2-gram-shape-40-mifuzz-0-4.rdb  has
  (gram-classify-greedy-mifuzz star-obj 3.0 0 4)

Same bad shit as before.
--- Greedy-checking next 200 items
---------Bingo! Dist=6.2186 for class "of to" -- "in"
---------Bingo! Dist=3.4062 for class "of to" -- "he"
---------Bingo! Dist=4.1679 for class "of to" -- "it"
---------Bingo! Dist=5.7443 for class "of to" -- "that"
---------Bingo! Dist=5.6019 for class "of to" -- "said"

Clearly, 3 is too small.

* r2-gram-shape-40-mifuzz-5-0-4.rdb  has
  (gram-classify-greedy-mifuzz star-obj 5.0 0 4)

Crap again. Nothing happens for the first 33 words, and then:

---------Bingo! Dist=5.5520 for word "be" -- "have"
---------Bingo! Dist=5.6404 for class "be have" -- "him"
---------Bingo! Dist=6.3487 for class "be have" -- "this"
---------Bingo! Dist=7.0597 for class "be have" -- "all"
---------Bingo! Dist=7.4523 for class "be have" -- "her"
---------Bingo! Dist=5.8003 for class "be have" -- "if"
---------Bingo! Dist=8.1334 for class "be have" -- "me"
---------Bingo! Dist=6.3648 for class "be have" -- "work"

So ... be-have looks plausible, and then everything else piled on is
crap. Crapity crap. What's happening?

========================================================================
