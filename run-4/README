
Sept 2021
---------
This is the working directory for examining deep trimming of a dataset.
All testing is done on the `run-1-en_mpg-tranche-123.rdb` dataset.
To avoid accidental data destruction/confusion, all work is done here
in the run-4 directory.

Results are reported in diary Part Three, title "Expt-4 Trimming Sept 2021"

The scripts in this directory were also used to prepare teh "clean"
trimmed files in run-1.


Datasets:
* r4-mpg-marg.rdb -- copy of full run-1-en_mpg-tranche-123.rdb set,
      with marginals for Sections only, no CrossSections.
      Computed with `guile -l run-common/marginals-mst.scm`
  Actually, this is a copy of `r3-mpg-marg.rdb` because I think that
  dataset was never corrupted, and its already got the marginals in it.
  ... It has the support marginals. It does not have the centrality
  report marginals.

* r4-trim*djmi.rdb -- these are re-trimmed using the latest and greatest
  trim scripts in `cleanup.scm`. Then they get the word-disjunct MI
  computed. This is documented in Diary Part Six.

* r4-mpg-djmi.rdb -- A copy of r4-mpg-marg.rdb with the word-disjunct
  MI batched (batch-all-pair-mi)  This required 106MB RAM to accomplish
  this! Ouch!   Anyway, its "clean" in that the `cleanup.scm` scripts
  passed it and found no problems.


Notes:
Do as the README suggests:
   (cog-rocks-open "rocks:///home/ubuntu/data/r4-mpg-marg.rdb")
   (define pca (make-pseudo-cset-api))
   (define psa (add-pair-stars pca))
   (psa 'fetch-pairs)        ;; Load the dataset
	(cog-close storage-node)

Rocks: initial aid=138920739
Elapsed time to load csets: 6796 secs
Elapsed time to load csets: 6606 secs  second time...
Elapsed time to load csets: 6640 secs
(print-matrix-summary-report psa) says : no cached data! Yikes!
RSS is 60.6 GB


The ../run-3/README file explains how to create filtered subsets.

We'll do this like so:

((add-support-compute psa) 'cache-all)
((make-central-compute psa) 'cache-all)
(print-matrix-summary-report psa)

; The correct filer.
	(define (zsto a b c fi)
		(define fsa (add-subtotal-filter psa a b c #f))
		(define zfa (add-zero-filter fsa #f))
		(define lfa (add-linkage-filter zfa))
		(define fso (make-store lfa))
		(define now (current-time))
		(cog-rocks-open (string-append "rocks:///home/ubuntu/data/" fi))
		(fso 'store-all-elts)	  ;; Do NOT store the marginals!
		(cog-rocks-close)
		(format #t "Total store time = ~A secs\n" (- (current-time) now))
	)

(zsto 10 4 2 "r4-zfil-10-4-2-pass-1.rdb")
After zsto its 81.2 GB RSS

Found 473877 pairs in 7326 secs
Stored 100000 of 473877 pairs in 25 secs (4000 pairs/sec)
Stored 200000 of 473877 pairs in 23 secs (4348 pairs/sec)
Stored 300000 of 473877 pairs in 23 secs (4348 pairs/sec)
Stored 400000 of 473877 pairs in 25 secs (4000 pairs/sec)
Done storing 473877 pairs in 115 secs
Total store time = 7441 secs

Do it again, but this time load pass 1 to create pass 2.
((add-support-compute psa) 'cache-all)
(zsto 10 4 2 "r4-zfil-10-4-2-pass-2.rdb")

(zsto 10 4 2 "r4-zfil-10-4-2-pass-3.rdb")
(zsto 10 4 2 "r4-zfil-10-4-2-pass-4.rdb")

(define (zall a b c fi)
   (define pca (make-pseudo-cset-api))
   (define psa (add-pair-stars pca))
   (psa 'fetch-pairs)        ;; Load the dataset
	(cog-close storage-node)

	((add-support-compute psa) 'cache-all)
	((make-central-compute psa) 'cache-all)
	(print-matrix-summary-report psa)

	(define fsa (add-subtotal-filter psa a b c #f))
	(define zfa (add-zero-filter fsa #f))
	(define lfa (add-linkage-filter zfa))
	(define fso (make-store lfa))
	(define now (current-time))
	(cog-rocks-open (string-append "rocks:///home/ubuntu/data/" fi))
	(fso 'store-all-elts)	  ;; Do NOT store the marginals!
	(cog-rocks-close)
	(format #t "Total store time = ~A secs\n" (- (current-time) now))
)

(zall 10 4 2 "r4-zfil-10-4-2-pass-5.rdb")
(zall 10 4 2 "r4-zfil-10-4-2-pass-6.rdb")
(zall 10 4 2 "r4-zfil-10-4-2-pass-7.rdb")
(zall 10 4 2 "r4-zfil-10-4-2-pass-8.rdb")

The below works in theory but is going to the a cpu killer.
So we don't do it.
(define (filt star-obj a b c)
	(define fsa (add-subtotal-filter star-obj a b c #t))
	(define zfa (add-zero-filter fsa #t))
	(define lfa (add-linkage-filter zfa))

	((add-support-compute lfa) 'cache-all)
	((make-central-compute lfa) 'cache-all)
	(print-matrix-summary-report lfa)

	; Return the filter object
	lfa
)

Trim!
((PredicateNode . 8) (ListLink . 26076503) (AnyNode . 2) (Connector .  725460) (ConnectorDir . 2) (ConnectorSeq . 25698949) (Section .  28436901) (AnchorNode . 1) (SchemaNode . 1) (RocksStorageNode . 1) (WordNode . 377553))

(cog-rocks-open "rocks:///home/ubuntu/data/r4-trim-1-1-1.rdb")
(subtotal-trim psa 1 1 1)

Trimmed left basis in 1036 seconds.
Trimmed right basis in 21624 seconds.
Trimmed all pairs in 789 seconds.

((PredicateNode . 14) (ListLink . 2557528) (AnyNode . 2) (Connector .
379443) (ConnectorDir . 2) (ConnectorSeq . 2366262) (Section . 2208776)
(TypeNode . 2) (AnchorNode . 1) (SchemaNode . 1) (RocksStorageNode . 2)
(WordNode . 191265))

; Get a brand-new stars

   (define pca (make-pseudo-cset-api))
   (define psa (add-pair-stars pca))
	((add-support-compute psa) 'cache-all)
((make-store psa) 'store-all)


After save and restore:
((PredicateNode . 8) (ListLink . 1791931) (AnyNode . 2) (Connector . 157829) (ConnectorDir . 2) (ConnectorSeq . 1733439) (Section . 2208776) (AnchorNode . 1) (SchemaNode . 1) (RocksStorageNode . 1) (WordNode . 102656))

pass 2:
Trimmed left basis in 49 seconds.
Trimmed right basis in 210 seconds.
Trimmed all pairs in 83 seconds.
Seems idempotent.

Because we forgot to trim-linkage first. If we trim-linkage, then:

((PredicateNode . 14) (ListLink . 1659114) (AnyNode . 2) (Connector .
157829) (ConnectorDir . 2) (ConnectorSeq . 1600622) (Section . 2072472)
(TypeNode . 2) (AnchorNode . 1) (SchemaNode . 1) (RocksStorageNode . 2)
(WordNode . 102656))

(for-each
   (lambda (base)
      (if (and (cog-atom? base) (equal? 0 (cog-incoming-size base)))
         (cog-delete-recursive! base)))
      (cog-get-atoms 'WordNode))

----------------------------
Lets try some automation.  This is the "Golden" code for trimming.

   (define pca (make-pseudo-cset-api))
   (define psa (add-pair-stars pca))
   (psa 'fetch-pairs)        ;; Load the dataset


(define (iter-trim a b c)
	(define pca (make-pseudo-cset-api))
	(define psa (add-pair-stars pca))
	(subtotal-trim psa a b c)
	(trim-linkage psa)
	(set! pca (make-pseudo-cset-api))
	(set! psa (add-pair-stars pca))
	((add-support-compute psa) 'cache-all)
	(print-matrix-summary-report psa)
	(inexact->exact (round ((add-support-api psa) 'total-support-left)))
)

; Same as above, but trims support.
(define (supp-trim a b c)
	(define pca (make-pseudo-cset-api))
	(define psa (add-pair-stars pca))
	(support-trim psa a b c)
	(trim-linkage psa)
	(set! pca (make-pseudo-cset-api))
	(set! psa (add-pair-stars pca))
	((add-support-compute psa) 'cache-all)
	(print-matrix-summary-report psa)
	(inexact->exact (round ((add-support-api psa) 'total-support-left)))
)

((batch-transpose psa) 'mmt-marginals)

====================================================
WTF. After trimming, and loading the trimmed file, there are .. word-nodes,
that got loaded ... and are not a part of the matrix.  How'd that happen?

Well, all that trimming seems to fail to trim some of the marginals
that were just hanging out.  So we do that by hand:

(for-each
	(lambda (base)
		(if (and (cog-atom? base) (equal? 0 (cog-incoming-size-by-type base 'Section)))
			(cog-delete-recursive! base)))
      (cog-get-atoms 'ConnectorSeq))

(for-each
	(lambda (base)
		(if (and (cog-atom? base) (equal? 0 (cog-incoming-size-by-type base 'Section)))
			(cog-delete-recursive! base)))
      (cog-get-atoms 'Word))

